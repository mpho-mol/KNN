K-Nearest Neighbors (KNN) is an effective supervised machine learning algorithm that makes predictions based on the majority class or the average of the K-nearest data points in classification and regression tasks respectively. In memory, the algorithm stores the entire training dataset consisting of input feature vectors and their corresponding class labels (for classification) or target values (for regression). The algorithm identifies the K-nearest neighbors to a given new, unseen input data point from the training dataset. The "distance" between points is often calculated using the Euclidean distance. For classification, the algorithm assigns the class label that is most common among the K-nearest neighbors to the new data point. And for  regression,
the algorithm calculates the average of the target values of the K-nearest neighbors and assigns that as the predicted target value for the new data point. Before executing the algorithm, the K value representing the number of neighbors to consider when making predictions, is a hyperparameter that needs to be indicated. It represents the number of neighbors to consider when making predictions. The choice of K and the scale of features can affect the algorithm's performance.

This aim of this task was to apply the KNN algorithm. This was done in Jupyter notebook using the Iris.csv file. Firstly, the libraries and dataset were imported then brief data analysis was carried out using the describe() and value_counts() functions. Then visualisations were plotted. The input and output values were determined in order to create the training and testing datasets. Because the KNN algorithm is scale-feature sensitive, the data was normalised. A KNN classifier was created for 5 neighbourhood and the testing data was predicted and an accuracy score, confusion matrix and classification report were determined. The exact same process was repeated for 50 neighbourhoods (K=50).
